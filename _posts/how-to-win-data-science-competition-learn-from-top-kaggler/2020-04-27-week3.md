---
layout: post
title: Week 3
---

## WEEK 3

### Metric optimization

### Regression metric:
  - MSE, RMSE, R-squared
  [](/how-to-win-data-science-competition-learn-from-top-kaggler/images/R_square_metric.png)
  - MAE is more robust, less sensitive with outliers than MSE. If unexpected values should be cared, then use MSE.
  - (R)MSPE (Mean Squared Percentage Error), MAPE (Mean Absolute Percentage Error)
  [](/how-to-win-data-science-competition-learn-from-top-kaggler/images/mspe_mape.png)
  - (R)MSLE (Root Mean Square Logarithmic Error)
  [](/how-to-win-data-science-competition-learn-from-top-kaggler/images/rmsle_metric.png)

--> MSE, RMSE prone to mean value of target variable; MAE prone to median value of target variable; (R)MSPE prone to weighted (1/y_i) mean value of target variable; MAPE prone to weighted (1/y_i) median value of target variable; (R)MSLE prone to mean value of target variable in logarithms space (which lead to more robust with outlier than MSE, RMSE).

#### Classification metric
  - Accuracy: prone to most frequent value of target variable (most frequent class).
  - AUC
  - logloss (cross-entropy)
  - [Cohen's (Quadratic weighted) Kappa](https://www.coursera.org/learn/competitive-data-science/lecture/EhJzY/classification-metrics-review): Commonly use on kaggle competition.
    
  
#### Mean Encoding
- Encode the catergorical features
- Encode the categories based on the ratio of occurrence of the positive class in the target variable
- i.e: assume binary classification task, cities features has 5 rows value: Hanoi. Correspond with these 5 rows, there are 3 positive value (label=1) --> encode Hanoi --> 3/5 = 0.6

- Good for GBDT algs.
- Why it works: 
  - separate zeros from ones.
- **Carefully with data leakage!!!**
- Ways to use mean encoding: [](/how-to-win-data-science-competition-learn-from-top-kaggler/images/mean_encoding.png)
- Notes: 
  - Always compute mean value (i.e above: 0.6) on the training set and then apply to valid/test set.
  - Mean encoding leads to overfitting --> need regulalization!!!
- Regulalization:
  - Cross-validation (usually 5-folds) loop inside training data. Note that each loop we have to compute mean encoding, cannot apply pipeline here (i.e cannot apply mean encoding then call cross_validation).
  - Smoothing: 
    - Good when categories have a lot of data points, and vice versa.
    - Need to combine with other regulalization approach! (i.e CV loop)
    [](/how-to-win-data-science-competition-learn-from-top-kaggler/images/smoothing_regulalization.png)
    
  - Add Noise
    - Degrade the quality of encoding.
    - Usually use with Leave-One-Out 
  - Expanding mean: using only rows from 0--> (n-1) to calculate encoding for row n.
  [](/how-to-win-data-science-competition-learn-from-top-kaggler/images/expanding_mean_encoding.png)
    - Least amount of leakage
    - No hyper parameter
    - Irregular encoding quality --> can permulate data, compute expanding mean encoding serveral times then average them.
    - Be used in CatBoost 
  
  --> Use CV loop or Expanding mean encoding in practical tasks.  
  
- Apply Mean Encoding
  - Multilabel classification: each value is a N-dimentional vector (N-the number of class)
  - [Many-to-many relationship](https://www.coursera.org/learn/competitive-data-science/lecture/D09Jb/extensions-and-generalizations)
  - Time series: not quite understand
  - Interactions & Numerical features: binning --> categorical. *Not quite understand, need to dig deeper*.
  - Notes: 
    - Always split train/valid set **before** encoding. Compute mean encoding in train set, map it to valid set.
    - At submission step, compute all original train set, apply it to test set.`
  
### Reference: [WEEK 3](https://www.coursera.org/learn/competitive-data-science/home/week/3) 
