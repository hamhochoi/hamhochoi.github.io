---
layout: post
title: "Week 2"
---

## WEEK 2

### EDA
- Build inituition about data: 
  - Get domain knowledge
  - Checking if data is intuitive: (i.e: a typo of age=336). 
  - Understand how data was generated (i.e: train and test set were generated with different algorithms --> **test set distribution is differ from train set distribution** --> cannot split train set into train-valid and use this validation set to validate). 
    - We can check this by see the correlation between validation score and the leaderboard score.

- Exploring anomalized data (advanced topic!)
  - Some organization want to hide their data by encoded text.
  - Can see the important of each feature and plot them (i.e RandomForest: plt.plot(model.feature_importances_))
  
- Exploring individual feature
```
  - df.dtypes
  - df.info()
  - x.value_counts()
  - x.isnull()
```
  
- Visualization  
  - Explore individual feature: **Never conclude by using only one plot. If you have a hypothesis, try difference plots to prove it.** 
    - Plots: ``` plt.plot(x) ```
    - Histograms: ``` plt.hist(x) ```
    - Statistics: ```df.describe(); x.mean(); x.var(); x.value_counts(); x.isnull()```
  - Explore feature correlations
    - Scatter plots: ```plt.scatter(x1, x2); pd.scatter_matrix(df); df.corr(), plt.matshow(...)```; grouping/clustering  ```df.mean().sort_values().plot(style='.')``` ![](/how-to-win-data-science-competition-learn-from-top-kaggler/images/groups.png)
    - Correlation plots
    - Plot (index vs feature statistics) 
    ![](/how-to-win-data-science-competition-learn-from-top-kaggler/images/plot.png)
    - ...
    
- Data Cleaning
  - Constant feature: ```traintest.nunique(axis==1) == 1```; a constant value in train set but a different constant value in test set --> remove all these features. 
  - Duplicate feature: 
    - numerical: ``` traintest.T.drop_dupplicates() ```; 
    - categorical: 2 features dupplicates but values have different names.
    ![](/how-to-win-data-science-competition-learn-from-top-kaggler/images/dupplicate_features.png)
  - Duplicate rows: find dupplicate rows, understand why it is dupplicated (random or a mistake in coding?)
  - Check if data is shuffled: plot target feature with row index. It can tell about data leakage!
  - **Visualization everything possible about dataset!**
   
### EDA Example: [notebook](https://www.coursera.org/learn/competitive-data-science/notebook/fhzQR/notebook-for-the-screencast); [video](https://www.coursera.org/learn/competitive-data-science/lecture/Cf3nS/springleaf-competition-eda-ii)
- Check if number of isnull() each row/column have some partern. If it does, then the data is not shuffled --> order of rows/columns might be matter --> Take 'ID' as a feature. 
[](/how-to-win-data-science-competition-learn-from-top-kaggler/images/id_feature.png) 
[](/how-to-win-data-science-competition-learn-from-top-kaggler/images/column_feature.png)
- If there are so many features, the first thing to do is determine the types of data.
- Check for dupplicate columns.
- Could use sorted correlation matrix to group features.
[](/how-to-win-data-science-competition-learn-from-top-kaggler/images/corr_matrix.png)

### Validation

- Validation Strategies: 
  - Holdout: train/valid/test
  - K-fold: cross validation
  - Stratification: K-fold and the folds are selected so that the mean response value is approximately equal in all the folds
    - Small dataset
    - Unbalance dataset
    - Multilabel classification
  - Leave-one-out: N-fold (N _ number of samples)

- Data split strategies
  - Random, row-based
  - Time-based
  - By ID
  - Combined 
--> Set up your validation strategies to mimic the train/test split of competition.

- Problems may occur:
  - Validation stage
    - too little data
    - too diverse and inconsistent data
--> K-fold
  - Submission stage
    - Check if leaderboard score correlate with validation score
      0. If we have quite different scores in K-fold --> compute mean, standard diviation of scores and check if leaderboard score lie in the range. Also, tune your model on one split and evaluate score on others.
      1. too little data in public leaderboard --> our model is ok.
      2. train and test data come from different distribution (i.e train data consist all men info, test data consist all women info; men info distribution is different from women info distribution).
        --> Leaderboard probing (see in data leakage sector!)
    - **Make sure validation set mimics the test set!**
[](/how-to-win-data-science-competition-learn-from-top-kaggler/images/train_set_diff_distribution.png)

  - LB shuffle: changes in the possition of paticipant are very much. Cause of LB shuffle:
    - Randomness: The data very randomness so that scores are highly random.
    - Little amount of data: dataset is too small
    - Different train/test set distribution: High public leaderboard score but lower private leaderboard score. --> overfitting.

### [Data Leakage](https://www.coursera.org/learn/competitive-data-science/lecture/Uxcm1/expedia-challenge) (Too difficult!!!) --> Ignore for now, learn in future!!!!
- Leak in time series
  - Data contain information about future.
  - Unexpected information
    - Meta data: change creation date, resize image, ...
    - Information from IDs
    - Row order

- [Leaderboard probing](https://www.coursera.org/learn/competitive-data-science/lecture/0tkKf/leaderboard-probing-and-examples-of-rare-data-leaks)


### Reference
- [Week 2](https://www.coursera.org/learn/competitive-data-science/home/week/2)



